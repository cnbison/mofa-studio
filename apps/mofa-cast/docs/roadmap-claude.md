# MoFA Cast - Technical Roadmap (Claude Analysis)

> Generated by Claude Code - January 2026
> **Last Updated**: 2026-01-08

---

## üìä Project Progress (as of 2026-01-08)

### ‚úÖ Completed

#### Phase 0: Infrastructure & UI Framework
- **Project Structure**: Created `apps/mofa-cast/` with proper Cargo workspace integration
- **Dependencies**: Configured all required crates (makepad-widgets, tokio, reqwest, etc.)
- **App Registration**: Implemented `MofaApp` trait and registered with MoFA Studio shell
- **UI Implementation**: Complete `CastScreen` widget with all planned components
  - Split-view editor (original | refined)
  - Left panel with import and speaker sections
  - Right panel with control buttons and editor
  - Dark mode support throughout
- **Shell Integration**:
  - Added to `mofa-studio-shell/Cargo.toml`
  - Registered in `LiveRegister` system
  - Sidebar navigation with "MoFA Cast" button
  - Visibility toggling for page switching
- **Icon Resource**: Created `cast.svg` icon
- **Documentation**: Organized all docs in `docs/` directory with updated content
- **Build Status**: ‚úÖ Successful compilation with no errors

#### Estimated Time Saved
- UI Framework: Originally estimated 5-6 days ‚Üí **Completed in 1 day**
- Due to: Clear architecture, existing mofa-fm reference, systematic approach

### üöß In Progress (Next)

#### Phase 1: Core Functionality (P0.1 - P0.5)
1. **Transcript Parsing** (4-5 days)
   - Plain text, JSON, Markdown parsers
   - Auto-detection
2. **AI Script Refinement** (5-7 days)
   - OpenAI/Claude API integration
   - Streaming responses
3. **Batch TTS Synthesis** (6-8 days)
   - Dora dataflow integration
   - Parallel workers
4. **Audio Mixing** (4-5 days)
   - Concatenation, normalization
   - MP3 export

**Total Remaining**: ~19-25 days

### üìà Confidence Level
- **Original**: 8/10
- **Current**: 9/10 (increased after successful UI implementation)

---

## Executive Summary

**MoFA Cast** is a **medium-complexity batch processing app** that transforms chat transcripts into podcast audio.

**Feasibility Score**: 8/10 (Achievable with standard techniques)

| Aspect | Difficulty | Status | Notes |
|--------|-----------|--------|-------|
| UI Implementation | 5/10 | ‚úÖ Complete | Standard editor layout |
| Transcript Parsing | 6/10 | ‚è≥ Next | Regex + JSON parsing |
| AI Refinement | 7/10 | ‚è≥ Planned | API integration, streaming |
| Batch TTS | 7/10 | ‚è≥ Planned | Parallel processing, queue management |
| Audio Mixing | 6/10 | ‚è≥ Planned | Standard audio libraries |

**Key Advantage**: No real-time constraints (batch processing is forgiving)

---

## Architectural Strengths

### 1. Batch Processing Model ‚úÖ

**Why This Is Easier**:
- No latency requirements (can take 5 minutes for 30min podcast)
- Can retry failures without user noticing
- Easier to test (deterministic outputs)
- No complex state synchronization

**Contrast with mofa-call**:
| Aspect | mofa-call (real-time) | mofa-cast (batch) |
|--------|---------------------|------------------|
| Latency | <500ms critical | Minutes acceptable |
| Complexity | High (streaming) | Medium (queue) |
| Error handling | Must be instant | Can retry |
| Testing | Hard (timing-sensitive) | Easy (deterministic) |

---

### 2. Parser Design Pattern üìÑ

**Challenge**: Support multiple chat formats

**Recommended Pattern**:
```rust
pub trait TranscriptParser {
    fn parse(&self, content: &str) -> Result<Transcript>;
    fn can_parse(&self, content: &str) -> bool;
}

// Factory pattern for auto-detection
pub struct ParserFactory {
    parsers: Vec<Box<dyn TranscriptParser>>,
}

impl ParserFactory {
    pub fn parse_auto(&self, content: &str) -> Result<Transcript> {
        for parser in &self.parsers {
            if parser.can_parse(content) {
                return parser.parse(content);
            }
        }
        Err(NoMatchingParser)
    }
}
```

**Implementation Priority**:
1. **PlainTextParser** (easiest, covers 70% of use cases)
2. **JsonParser** (structured, easy to parse)
3. **MarkdownParser** (for GitHub discussions)
4. **WhatsAppParser** (popular export format)

---

### 3. AI Refinement Strategy ü§ñ

**Prompt Engineering is Critical**:

```markdown
System: You are a podcast script editor. Transform casual chat into engaging podcast dialog.

User:
Original transcript:
Alice: hey whats up
Bob: not much u?
Alice: working on that thing we talked about
Bob: cool

Transform this into a podcast script with:
1. Natural transitions
2. Clear speaker attribution
3. Expanded context
4. Engaging tone

Format:
[Host] Brief introduction if needed.
[Alice] Full sentence dialog.
[Bob] Response.
```

**Output Example**:
```
[Host] Alice and Bob catch up on a recent project.
[Alice] I've been working on the project we discussed earlier.
[Bob] That's great! How's it progressing?
```

**Optimization**:
- Use **streaming** for immediate feedback (word-by-word display)
- Cache results to avoid re-processing
- Allow user to regenerate specific segments

---

### 4. Parallel TTS Architecture üéôÔ∏è

**Challenge**: Synthesize 100+ segments efficiently

**Solution**: Worker pool pattern

```rust
pub struct TTSWorkerPool {
    workers: Vec<TTSWorker>,
    work_queue: Sender<Segment>,
    result_queue: Receiver<AudioFile>,
}

impl TTSWorkerPool {
    pub fn new(num_workers: usize) -> Self {
        let (work_tx, work_rx) = crossbeam_channel::unbounded();
        let (result_tx, result_rx) = crossbeam_channel::unbounded();

        let workers = (0..num_workers)
            .map(|id| TTSWorker::spawn(id, work_rx.clone(), result_tx.clone()))
            .collect();

        Self { workers, work_queue: work_tx, result_queue: result_rx }
    }

    pub fn synthesize_batch(&self, segments: Vec<Segment>) -> Vec<AudioFile> {
        // Queue all work
        for seg in segments {
            self.work_queue.send(seg).unwrap();
        }

        // Collect results
        let mut results = Vec::new();
        for _ in 0..segments.len() {
            results.push(self.result_queue.recv().unwrap());
        }
        results
    }
}
```

**Performance Calculation**:
- 1 speaker: 300 chars/min (TTS RTF=0.76)
- 2 speakers parallel: 600 chars/min
- Typical chat (10,000 chars): ~17 minutes with 2 workers

---

### 5. Audio Mixing Pipeline üéµ

**Requirements**:
1. Concatenate segments in order
2. Normalize volume (prevent clipping)
3. Add silence between speakers (0.5s)
4. Export as MP3 (128kbps)

**Libraries**:
```rust
// Read WAV files
use hound::{WavReader, WavWriter};

// Decode various formats
use symphonia::default::get_probe;

// Resample (if needed)
use rubato::{Resampler, SincFixedIn};

// Encode MP3
use lame_sys::lame_encode_buffer_ieee_float;
```

**Pipeline**:
```
Load WAV segments
    ‚Üì
Normalize peak levels (-3dB headroom)
    ‚Üì
Add 0.5s silence between
    ‚Üì
Concatenate to single buffer
    ‚Üì
Encode to MP3 (128kbps VBR)
    ‚Üì
Write file with ID3 tags
```

---

## Implementation Recommendations

### Phase 1: Minimal Parser + Static UI (Low Risk)

**Goal**: Import plain text, display in UI

**Deliverable**:
```rust
// Input
let chat = "Alice: Hello\nBob: Hi there";

// Parse
let transcript = PlainTextParser::new().parse(chat)?;

// Display
screen.display_transcript(transcript);
```

**Why Start Here**: Validates end-to-end data flow without complex dependencies

---

### Phase 2: AI Refinement (Medium Risk)

**Goal**: Call OpenAI API, display refined script

**Challenges**:
- Async API call (use tokio + channels)
- Streaming response (update UI word-by-word)
- Rate limiting (handle 429 errors)

**Deliverable**: "Refine" button generates improved script in 10-30s

---

### Phase 3: Batch TTS (High Risk)

**Goal**: Generate audio for each segment

**Critical Decision**: Use Dora dataflow or direct library?

**Option A: Dora Dataflow** (recommended)
- ‚úÖ Consistent with other apps
- ‚úÖ Can swap TTS engines easily
- ‚úÖ Monitoring and logging built-in
- ‚ö†Ô∏è Requires dataflow management

**Option B: Direct TTS Library** (simpler)
- ‚úÖ Fewer dependencies
- ‚úÖ Easier debugging
- ‚ö†Ô∏è Tight coupling
- ‚ö†Ô∏è Harder to scale (add voices)

**Recommendation**: Use Dora, follow mofa-explainer async pattern

---

### Phase 4: Audio Mixing (Low Risk)

**Goal**: Combine segments, export MP3

**Libraries are mature**: hound, symphonia, lame are production-ready

**Testing Strategy**:
```bash
# Generate test segments
echo "Hello world" | tts_cli --voice alice > seg1.wav
echo "How are you" | tts_cli --voice bob > seg2.wav

# Mix
cargo run --bin audio_mixer -- seg1.wav seg2.wav output.mp3

# Verify (listen, check duration)
ffprobe output.mp3
```

---

## Technical Challenges & Solutions

### Challenge 1: Large Transcripts (10,000+ messages)

**Problem**: UI freezes, parsing slow, memory usage high

**Solution**:
1. **Lazy loading**: Parse in chunks (1000 messages at a time)
2. **Virtual scrolling**: Only render visible messages
3. **Background parsing**: Parse in separate thread, update UI incrementally

```rust
// Incremental parsing
impl TranscriptParser {
    pub fn parse_incremental(&self, content: &str, chunk_size: usize) -> impl Iterator<Item = Message> {
        content.lines()
            .chunks(chunk_size)
            .map(|chunk| self.parse_chunk(chunk))
            .flatten()
    }
}
```

---

### Challenge 2: Speaker Identity Consistency

**Problem**: AI might change speaker names ("Alice" ‚Üí "Alice Smith")

**Solution**:
1. Extract unique speakers from original transcript
2. Lock speaker IDs in refined script
3. Use placeholder tokens: `[SPEAKER_0]`, `[SPEAKER_1]`
4. Replace placeholders with actual names after AI

```rust
// Pre-processing
let speakers = extract_speakers(&transcript);  // ["Alice", "Bob"]
let prompt = format!(
    "Transform this chat. Use [SPEAKER_0] for Alice, [SPEAKER_1] for Bob.\n{}",
    transcript
);

// Post-processing
let refined = ai_response
    .replace("[SPEAKER_0]", "Alice")
    .replace("[SPEAKER_1]", "Bob");
```

---

### Challenge 3: Audio Synchronization

**Problem**: Segment files may be generated out of order (parallel workers)

**Solution**: Include sequence number in filename

```rust
// Generate with sequence
for (i, segment) in segments.iter().enumerate() {
    let filename = format!("segment_{:04}_{}.wav", i, segment.speaker_id);
    synthesize_to_file(segment.text, filename);
}

// Sort before concatenating
let mut audio_files: Vec<_> = glob("segment_*.wav").collect();
audio_files.sort();  // Lexicographic sort works with zero-padding

for file in audio_files {
    audio_buffer.append(read_wav(file));
}
```

---

## Alternative Architectures Considered

### Option A: Real-time Refinement (Rejected)

**Idea**: Refine script as user types

**Pros**: Instant feedback
**Cons**: Too many API calls, distracting, expensive

**Decision**: Stick with on-demand refinement (user clicks button)

---

### Option B: Cloud Processing (Future)

**Idea**: Upload transcript, process on server, download result

**Pros**: No local GPU needed, faster TTS
**Cons**: Privacy concerns, network dependency

**Decision**: MVP is local-only, consider cloud as P2 feature

---

## Success Criteria Validation

| Metric | Target | How to Measure |
|--------|--------|----------------|
| Parse accuracy | 95% | Test on 20 real chat exports, count errors |
| Refinement quality | 4/5 stars | User rating (subjective) |
| TTS speed | <5min for 30min | Benchmark with standard script |
| Audio quality | No artifacts | Listen test, check waveform for clicks |
| Memory usage | <500MB | Profile during large transcript (1000+ msgs) |

---

## Estimated Timeline

| Phase | Duration | Confidence |
|-------|----------|-----------|
| P0.1 Parsing | 4-5 days | High |
| P0.2 UI | 5-6 days | High |
| P0.3 AI Refinement | 5-7 days | Medium (API complexity) |
| P0.4 Batch TTS | 6-8 days | Medium (Dora integration) |
| P0.5 Audio Mixing | 4-5 days | High (mature libraries) |
| **Total MVP** | **24-31 days** | |

**Buffer**: Add 20% for testing, debugging, unexpected issues ‚Üí **29-37 days**

---

## Conclusion

**MoFA Cast is highly feasible** with:
- Medium complexity (batch processing is forgiving)
- Mature libraries (audio, parsers)
- Clear architectural patterns (worker pool, channels)

**Recommended Approach**:
1. Start with **PlainTextParser** (quick win)
2. Build **static UI** (validate layout)
3. Add **AI refinement** (core value)
4. Implement **batch TTS** (most complex part)
5. Add **audio mixing** (polish)

**Confidence Level**: 8/10 (lower risk than mofa-call, higher than mofa-github)

---

**Last Updated**: 2026-01-07
**Recommendation**: ‚úÖ Proceed with phased implementation, prioritize parser diversity
